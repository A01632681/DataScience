{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigment 2 <br>\n",
    "##### Registration number: 22018887<br>\n",
    "This notebook is focused on how to read the data from the 'Stress-Predict-Dataset' GitHub repository in order to process it and with the help of machine learning algorithms, try to determine when a student may have stress and when not. All the signals where processed from raw and are conformed by 6 different signals of 35 participants. The signals that will be used are mentioned below: <br> \n",
    "- EDA = Electrodermal Activity <br>\n",
    "- HR = Heart Rate <br>\n",
    "- Temp = Temperature <br> <br>\n",
    "To begin, the appropriate libraries will be imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing\n",
    "First create some variables to iterate over the files and signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['S' + str(format(i, '02')) for i in range(36)] # + 1 because range starts at 0\n",
    "numbers = [i for i in range(len(names))] # Create a similar list to have the number of participant\n",
    "names.pop(0) # Pop the first element of the list to match the names of the files\n",
    "names.pop(0) # Pop again because participant number 1 signals are corrupted\n",
    "\n",
    "numbers.pop(0) # Pop the first element of the list to match the names of the files\n",
    "numbers.pop(0) # Pop again because participant number 1 signals are corrupted\n",
    "\n",
    "# Create an empty dictionary that will contain all the processed signals\n",
    "df_var = {}\n",
    "\n",
    "# Create a list of the variables and its columns\n",
    "variables = ['HR', 'EDA','TEMP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S02', 'S03', 'S04', 'S05']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will use the first 5 subject to test, to make it faster and easier\n",
    "# If we wanted to see all the subjects, we would comment the following lines\n",
    "del names[4:35]\n",
    "names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will import the timelogs dataframe, this contains all the data about the events where the student was in a stressfull environment or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import timelogs for visualization\n",
    "# This DataFrame contains the time logs for each subject responses\n",
    "timelogs = pd.read_csv('Time_logs.csv')\n",
    "timelogs = timelogs.drop(0, axis=0) # Drop the first row\n",
    "timelogs = timelogs.drop(['Session ID', 'Unnamed: 24'], axis=1) # Drop the columns that are not needed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once imported, all the hours and dates will be converted into UNIX timestamp, since all the signals in the repository works with this time style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the time logs to unix timestamps\n",
    "# This is done to be able to compare the time logs with the data\n",
    "for column in timelogs.columns:\n",
    "    unix_timestamps = [] # Create an empty list to store the unix timestamps\n",
    "    if column != 'Date' and column != 'S. ID.': # If the column is not the date or the subject ID\n",
    "        timelogs[column] = timelogs.Date + ' ' + timelogs[column] # Concatenate the date and the time\n",
    "        times = timelogs[column].values.tolist() \n",
    "        for date in times:\n",
    "            dt_obj = datetime.strptime(date, '%d-%b-%Y %H:%M') # Convert the string to datetime object\n",
    "            unix_timestamps.append(int(dt_obj.timestamp())) # Convert the datetime object to unix timestamp\n",
    "        timelogs[column] = unix_timestamps # Replace the values in the column with the unix timestamps\n",
    "timelogs = timelogs.drop(['Date', 'Start Time', 'End Time','Consent','Unnamed: 5'], axis=1) # Drop the columns that are not needed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions\n",
    "This next functions are used in the preprocessing and are applied to each signal, they are in charge of using the unix timestamp in order to label according to the timelogs dataframe events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ------- Functions ------- ##\n",
    "def prepare_data(df_old, signal):\n",
    "    '''\n",
    "    The function takes a DataFrame and a signal name as input and returns \n",
    "    a new DataFrame with the time for each value of the signal in UNIX and \n",
    "    a empty label column that will store if is stress or not.\n",
    "    '''\n",
    "    df_new = df_old.copy() # Create a copy of the DataFrame\n",
    "    start_time = df_new.iloc[0,0] # Get the start time of the signal\n",
    "    # Drop the first two rows because they are not needed\n",
    "    df_new.drop(0, axis=0, inplace=True) \n",
    "    df_new.drop(1, axis=0, inplace=True)\n",
    "    df_new.reset_index(drop=True, inplace=True) # Reset the index\n",
    "    df_new.columns = [signal] # Rename the column to the signal name\n",
    "\n",
    "    # If the signal is HR, use each value as the time for each value\n",
    "    # If the signal is not HR, use the time of the first value of each 4 values\n",
    "    # This is done because the other signals are sampled at 4Hz\n",
    "    if signal == 'HR':\n",
    "        df_new['time'] = [int(start_time+(i)) for i in range(len(df_new))]\n",
    "        df_new[\"label\"] = \"\"\n",
    "    else:\n",
    "        df_new['time'] = np.repeat(np.arange(len(df_new)// 4 +1)+int(start_time), 4)[:len(df_new)]\n",
    "        df_new[\"label\"] = \"\"\n",
    "    return df_new\n",
    "\n",
    "def label(df,subject_number):\n",
    "    '''\n",
    "    The function takes a DataFrame and a subject number as input and returns\n",
    "    a new DataFrame with the labels for each value of the signal depending on the\n",
    "    timelogs dataframe.\n",
    "    Not stress = 0\n",
    "    Stress = 1\n",
    "    '''\n",
    "    # Create a flag to know if the value is stress or not\n",
    "    flag = 0\n",
    "    index = 1\n",
    "    for column in range(timelogs.shape[1]):\n",
    "        next_index = index + 1 # Get the next index to compare the time logs\n",
    "        if index <= 17: # Check if the index is in the range of the timelogs\n",
    "            val1 = timelogs.iloc[subject_number-1, index] # Get the value of the timelog\n",
    "            val2 = timelogs.iloc[subject_number-1, next_index] # Get the value of the next timelog\n",
    "            if flag == 0: # A flag is used since the stress and not stress values are intercalated\n",
    "                df.loc[(df['time'] >= val1) & (df['time'] <= val2), 'label'] = 0 # Label all the values between the timelogs as not stress\n",
    "                flag = 1 \n",
    "            elif flag == 1:\n",
    "                df.loc[(df['time'] >= val1) & (df['time'] <= val2), 'label'] = 1 # Label all the values between the timelogs as stress\n",
    "                flag = 0\n",
    "        index += 2 # Increase the index by 2 to get the next timelog\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next loop is in charge of reading all the signals from the repository and apply the necessary preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ban = 0 # Create a flag to know if the DataFrame is the first or not\n",
    "for signal in variables:\n",
    "    for name in names:\n",
    "        indx = names.index(name) # Get the index of the name to get the number of the participant\n",
    "        # Read the csv file and store it in a DataFrame\n",
    "        df = pd.read_csv(f'https://raw.githubusercontent.com/italha-d/Stress-Predict-Dataset/main/Raw_data/{name}/{signal}.csv', header=None)\n",
    "        df_new = prepare_data(df, signal) # Prepare the DataFrame using the function\n",
    "        df_new = label(df_new, numbers[indx]) # Label the DataFrame using the function\n",
    "        # Replace the empty values with NaN and drop the NaN values\n",
    "        df_new = df_new.replace('', np.nan, inplace=False) \n",
    "        df_new = df_new.dropna(inplace=False)\n",
    "        if ban == 0: # If the DataFrame is the first, store it in the dictionary\n",
    "            df_var[signal] = df_new\n",
    "            ban = 1\n",
    "        else: # If the DataFrame is not the first, concatenate it with the previous one\n",
    "            df_var[signal] = pd.concat([df_var[signal], df_new], ignore_index=True)\n",
    "    ban = 0 # Reset the flag"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to visualize data, we can see a possible separation of stress and not stressed students in the plot, being the height of the IBI value the \"treshhold\".\n",
    "This is because, according to the paper, while there is more HR and less IBI, there is more stress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaxis = pd.DataFrame(df_var.get('IBI'))\n",
    "fig, axs = plt.subplots(5)\n",
    "fig.suptitle('IBI per subject')\n",
    "\n",
    "for i in names:\n",
    "    ylab = yaxis.loc[:1500,[f'IBI_ms_{i}']]\n",
    "    xaxis = np.linspace(init[names.index(i)], init[names.index(i)]+len(ylab), len(ylab))\n",
    "    axs[names.index(i)].plot(xaxis, ylab)\n",
    "    axs[names.index(i)].set_title(f'Subject {i}')\n",
    "    axs[names.index(i)].set_ylabel('IBI (ms)')\n",
    "    axs[names.index(i)].set_xlabel('Time (s)')\n",
    "    #axs[names.index(i)].set_xlim([init[names.index(i)], init[names.index(i)]+1500])\n",
    "    #axs[names.index(i)].set_ylim([0, 2000])\n",
    "    #axs[names.index(i)].grid()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to try to detect stress, a simple machine learning algorithm will be used, as well as a simple Neural Network and they will be compared after to check on the perfomance. \n",
    "First, it is necesary to separate each signal from the dictionary, as they are going to be used independently to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HR = pd.DataFrame.from_dict(df_var['HR'])\n",
    "df_EDA = pd.DataFrame.from_dict(df_var['EDA'])\n",
    "df_TEMP = pd.DataFrame.from_dict(df_var['TEMP'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working with timeseries, a normal approach of train-test-split can not be done since it will destroy the timeseries and we will lose information. So in order to work witrh this type of information, we can separate the information by groups according if they are for stress or not, and then apply a sliding window that will gather the data and be inputed into the ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column to indicate if the value correspond to a stress or not stress group\n",
    "df_EDA['group'] = (df_EDA['label'] != df_EDA['label'].shift()).cumsum()\n",
    "\n",
    "# Create a list of groups to exclude from the train set\n",
    "groups = list(df_EDA['group'].unique())\n",
    "# Randomly select 50% of the groups to exclude from the train set\n",
    "exclude_groups = np.random.choice(groups, size=int(0.5*len(groups)), replace=False)\n",
    "# Create a new column to indicate which rows are in the training set and which are in the test set\n",
    "df_EDA['set'] = np.where(df_EDA['group'].isin(exclude_groups), 'test', 'train')\n",
    "\n",
    "# Split the data into training and test sets\n",
    "x_train_df = df_EDA[df_EDA['set'] == 'train'].drop(columns=['label','time','group','set'])\n",
    "y_train_df = df_EDA[df_EDA['set'] == 'train'].drop(columns=['EDA','time','group','set'])\n",
    "x_test_df = df_EDA[df_EDA['set'] == 'test'].drop(columns=['label','time','group','set'])\n",
    "y_test_df = df_EDA[df_EDA['set'] == 'test'].drop(columns=['EDA','time','group','set'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the information is separated into train and test, the windowing can be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sliding window of size n for the training data\n",
    "n = 2\n",
    "# Create empty lists to store the data\n",
    "x_train_data = []\n",
    "y_train_data = []\n",
    "for i in range(len(x_train_df)-n+1): # We use -n+1 to avoid index out of range\n",
    "    # Apply the sliding window\n",
    "    x_window = x_train_df.iloc[i:i+n].values.flatten()\n",
    "    y_window = y_train_df.iloc[i:i+n].values.flatten()\n",
    "    # Store the values into the lists\n",
    "    x_train_data.append(x_window)\n",
    "    y_train_data.append(y_window)\n",
    "\n",
    "# Create a sliding window of same size n for the test data\n",
    "# Create empty lists to store the data\n",
    "x_test_data = []\n",
    "y_test_data = []\n",
    "for i in range(len(x_test_df)-n+1): # We use -n+1 to avoid the last window to be smaller than n\n",
    "    # Apply the sliding window\n",
    "    x_window = x_test_df.iloc[i:i+n].values.flatten()\n",
    "    y_window = y_test_df.iloc[i:i+n].values.flatten()\n",
    "    # Store the values into the lists\n",
    "    x_test_data.append(x_window)\n",
    "    y_test_data.append(y_window)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply the models we want to compare to the data, since it is already separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carlos\\AppData\\Local\\Temp\\ipykernel_11132\\3092782266.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  clf.fit(x_train_df, y_train_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.42      0.75      0.54      9600\n",
      "         1.0       0.47      0.17      0.25     12000\n",
      "\n",
      "    accuracy                           0.43     21600\n",
      "   macro avg       0.44      0.46      0.40     21600\n",
      "weighted avg       0.45      0.43      0.38     21600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=23)\n",
    "clf.fit(x_train_df, y_train_df)\n",
    "\n",
    "# Make predictions on the test data\n",
    "# test_features = [x for x in x_test_data]\n",
    "# test_labels = [y for y in y_test_data]\n",
    "predictions = clf.predict(x_test_df)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_df, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
